\documentclass{article}

\usepackage{amsmath}
\usepackage{biblatex}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{math_macro}
\usepackage{enumitem}

\bibliographystyle{plain}
\bibliography{20240612_report}

\title{Molgena}
\subtitle{Molecular Generator}
\author{Lorenzo Rutayisire}
\date{12 Jun 2024}

\begin{document}

\maketitle
\tableofcontents

\section{Introduction}

\textbf{Molgena} is a project on which I've worked between Feb 2024 and Jun 2024 for the ``AI for Bioinformatics`` university exam.
As for the days of writing (roughly 12 Jun 2024), \textbf{the project isn't complete} but already shows nice results worth to post.
Hopefully could be subject to future work if anyone is interested and/or I find some spare time.

\subsection{Background}

The task is Molecule Generation, that is the generation of novel molecules such that they
optimize a desired chemical property (as described in \href{https://tdcommons.ai/generation_tasks/molgen/}{Therapeutics Data Commons}).

To this end, I've considered the works listed below:

\begin{itemize}
\item ``Junction Tree Variational Autoencoder for Molecular Graph Generation`` \cite{jtvae2019}
\item ``Hierarchical Generation of Molecular Graphs using Structural Motifs`` \cite{hievae2020}
\item ``Learning to Extend Molecular Scaffolds with Structural Motifs`` \cite{microsoft2024}
\end{itemize}

I've only considered work that rely on the usage of GNNs. There are several works (previous and present) that make use
of Transformer architectures and work directly on SMILES strings, but I've excluded those.

The datasets used to perform this task are datasets consisting of chemically valid molecules, described as SMILES
strings. I've used ZINC, a dataset consisting of 249455 compounds.

Property optimization is then addressed by first constructing a latent space for molecules, then such space is mapped
to the property score function, to obtain a $y(z)$ function. We can now search for the latent vector that maximizes
such score (i.e. $z$ that maximizes $y$), and finally $z*$ is decoded to obtain the molecule. If the latent space
encodes molecules semantics properly, we can possibly search for molecules similar to a query molecule.

\section{Data preparation}

Before actually running Molgena (training or inference), it's required to prepare the data.
We can summarize the data preparation in the following steps:

\begin{enumerate}
\item Download the dataset
\item Split the dataset into training, validation, and test sets
\item Generate the Motif vocabulary
\item Filter training, validation, and test sets
\item Cache the mgraphs for all sets molecules
\end{enumerate}

\subsection{Dataset splitting and filtering}

As mentioned above, the dataset is ZINC and initially consists of 249455 molecules.
We \textbf{split} it randomly retaining 80\% molecules for the training set, 10\% molecules for the validation set, and 10\% molecules for the test set.

Then, we \textbf{filter} molecules in all sets such that:
\begin{itemize}
    \item the molecule can be decomposed into valid Motifs (training set is skipped)
    \item mgraph identity is valid for the molecule
\end{itemize}
Filtering is supposed to be done only after the Motif vocabulary is generated.

Finally our dataset looks like the following:
\begin{itemize}
    \item 199560 molecules for the training set (4 discarded)
    \item 24748 molecules for the validation set (198 discarded)
    \item 24736 molecules for the test set (210 discarded)
\end{itemize}

\subsection{Motifs and Motif vocabulary}

Following the work of \cite{jtvae2019}, \cite{hievae2020} and \cite{microsoft2024}, Molgena doesn't generate molecules
predicting bare atoms and bonds. Instead, the building bricks are Motifs, that are drawn from a Motif vocabulary, that is
constructed from the training set.

A \textbf{Motif} is a molecular substructure that is obtained from the training set.

The Motif vocabulary generation consists in iterating each training set molecule and breaking bonds if:
\begin{enumerate}
\item the bond connects two different rings
\item the bond connects a ring and a non-leaf atom (degree > 1)
\end{enumerate}
finally we obtain a list of \textbf{candidate Motifs} by searching for the connected components of the resulting molecular graph.
Once we iterated over all training set molecules, a candidate Motif is elected to be a Motif if it's frequent enough in
the training set (e.g. belongs to more than 100 molecules).

Since we want Motifs to cover all training set molecules, if a candidate Motif wasn't elected, we further decompose
it to atoms and rings by breaking all bonds outside rings and searching the connected components. Those will be elected to
Motifs.

Molegna vocabulary consists of 4331 Motifs.

\subsection{Motif graph (mgraph) and Mgraph caching}

A Motif graph (mgraph) of a molecule is a graph constructed on top of the molecule, whose nodes are Motifs.
It can thus be achieved only after the Motif vocabulary is built.
Motif graphs aims at simplifying the molecule's graph structure, and ease model predictions (e.g. avoid predicting rings).

In code, we refer to mgraph creation as \textbf{construction}, and the inverse operation (back to molecular graph) as \textbf{conversion}.
Mgraph construction/conversion must be feasible for a dataset molecule to be valid, and the result must lead to the same canonical SMILES.

\begin{lstlisting}[linewidth=\linewidth]
mol2 = mgraph_convert(mgraph_construct(mol1))
mol1 == mol2
\end{lstlisting}

We call the composition of these two functions \textbf{mgraph identity}.

Mgraph construction is a common operation during training/inference and rather slow.
To avoid it as much as possible, we \textbf{cache the mgraph} of already known molecules (all dataset molecules).

\section{Architecture}

Molgena architecture resembles MoLeR architecture \cite{microsoft2024}.
It's composed of 5 different modules, each addressing a specific task, and they operate together to incrementally construct (or optimize) an input molecule.
Molgena exhibits an Encoder module, that is responsible for mapping molecules to a latent space that encodes their semantics.

\begin{itemize}
\item EncodeMol
\item SelectMotifMlp
\item SelectAttachmentClusters
\item SelectAttachmentAtom
\item SelectAttachmentBondType
\end{itemize}

During experiments I've tried several versions of Molgena, the one I found working better was \textbf{molgena-2b}.
Below I describe the technical layout of each module.

\subsection{EncodeMol}

\begin{description}
\item[\textbf{Input}]
    A tensorized graph (either molecular graph or mgraph)
\item[\textbf{Output}]
    A latent vector representing the input graph,
    and nodes/edges hidden vectors that are generated after the propagation step.
\end{description}

This module is a GNN, and the Message Passing used is the same described by \cite{jtvae2019}.
Specifically, it consists of $T$ iterations to compute the edges' hidden vector $\bnu_{uv}$:

\begin{equation}
    \bnu_{uv}^{(t)} = \tau(\W_1^g \x_u + \W_2^g \x_{uv} + \W_3^g \sum_{w \in N(u) \backslash v} \nu_{wu}^{(t-1)})
\end{equation}

And one final step to calculate the nodes' hidden vector $\h_u$:

\begin{equation}
    \h_u = \tau(\U_1^g \x_u + \sum\nolimits_{v \in N(u)} \U_2^g \bnu_{vu}^{(T)})
\end{equation}

At this point, every node (or atom) will have developed a hidden vector that is dependant on its neighborhood within the graph.
We compute the final molecule's latent vector by taking a mean of all nodes' hidden vectors:

\begin{equation}
    \h_G = \sum_i \frac{\h_i}{\len{V}}
\end{equation}

This type of message passing, isn't provided by \href{https://pytorch-geometric.readthedocs.io/en/latest/}{pytorch-geometric},
and I didn't find easy to integrate it as a \href{https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html}{custom message passing network}.
Therefore, I've implemented it by scratch with pytorch. Graph data format and batching is \href{https://pytorch-geometric.readthedocs.io/en/latest/advanced/batching.html}{inspired on pytorch-geometric}.

The same module appears in two instances within Molgena: one for molecular graphs, the other for mgraphs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% SelectMotifMlp

\subsection{SelectMotifMlp}

\begin{description}
\item[\textbf{Input}]
    The partial molecule latent vector\\
    The target molecule latent vector
\item[\textbf{Output}]
    A distribution over the Motif vocabulary indicating how suitable is a Motif to be picked as next.
    An extra item is added to indicate the END of the generation.
    Therefore, the final output shape is $4331 + 1 = 4332$.
\end{description}

The module is an MLP with 3 hidden layers of dimensionality 384, 512, 1024.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% SelectAttachmentCluster

\subsection{SelectAttachmentClusters}

\begin{description}
\item[\textbf{Input}]
    The partial molecule mgraph\\
    The motif feature vector
    % TODO no target molecule ???
\item[\textbf{Output}]
    The output is a binary vector over the partial molecule mgraph nodes (or \textbf{clusters}).
    Thus one entry per cluster. It indicates whether a cluster participate or not to the attachment with
    the selected next Motif.
\end{description}

This module requires the partial molecule mgraph to have valid hidden vectors, and thus message passing to have run on it.
Afterwards, an MLP is applied on all clusters, paired with the Motif feature vector.
We apply a sigmoid on the output logit to obtain a binary value (picked or not picked).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% SelectAttachmentAtom

\subsection{SelectAttachmentAtom}

\begin{description}
\item[\textbf{Input}]
    Hidden vectors of \textbf{cluster1} nodes\\
    Latent representation for \textbf{cluster2}\\
    Target molecule latent vector
\item[\textbf{Output}]
    The output is a binary vector over the partial molecule mgraph nodes (or \textbf{clusters}).
    Thus one entry per cluster. It indicates whether a cluster participate or not to the attachment with
    the selected next Motif.
\end{description}

Selects a cluster1 atom to form the attachment with cluster2.

This module appears in two instances in Molgena: to select the attachment atom from the partial molecule cluster, and to select the atom on the Motif.\\
In the first instance, cluster1 is the cluster chosen by SelectAttachmentClusters, and cluster2 is the Motif.\\
In the second instance, cluster1 is the Motif, and cluster2 a latent vector calculated as the mean of cluster1 hidden vectors.
\textbf{Please note: the hidden vectors are computed by running message passing on the whole graph, while the mean is only on a subset of those}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% SelectAttachmentBondType

\subsection{SelectAttachmentBondType}

\begin{description}
\item[\textbf{Input}]
    Hidden vector for cluster1 atom\\
    Hidden vector for cluster2 atom\\
    The target molecule latent vector
\item[\textbf{Output}]
    A distribution over the 4 possible bond types: none, single/double/triple covalent.
\end{description}

This module is an MLP whose input are the hidden vectors for cluster1 and cluster2 atoms, and the target molecule latent vector.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Training

\section{Training}

We've trained Molgena on the \textbf{reconstruction task}.

The problem formulation is: given a molecule drawn from the dataset, we train our model to incrementally attach motifs together to reconstruct it.
The purpose of training on such task is to build a semantically meaningful latent space, where structurally similar molecules are near each other,
on which leverage to achieve the former property optimization task.

At every training inference, we take into consideration \textbf{a single attachment step}.
That is, drawn the molecule from the training set, we construct the mgraph for such molecule and then sample a subgraph of the mgraph.
The sampling is performed such that it's equally probable to sample a subgraph with any number of nodes (including an empty or a full graph).
Then, we generate annotations for all model's modules:
the next motif is randomly sampled (any that is missing from the mgraph subgraph),
the two attachment atoms, and the bond type.

Once the whole attachment step is annotated, the inference follows these steps:

\begin{lstlisting}
pmol = "partial molecule"
tmol = "target molecule"

pmol_mgraph = ConstructMgraph(pmol)
z_pmol = EncodeMol(pmol)
z_tmol = EncodeMol(tmol)
EncodeMol(pmol_mgraph)

motif = SelectMotifMlp(pmol)
z_motif = EncodeMol(motif)

cluster1 = SelectAttachmentClusters(pmol_mgraph, motif.feature_vector)
cluster1_atom = SelectAttachmentAtom(cluster1, z_motif, z_tmol)
motif_atom = SelectAttachmentAtom(motif, repr(cluster1), z_tmol)
bond_type = SelectAttachmentBondType(motif_atom, cluster2_atom)
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Loss function

\subsubsection{Loss function}

Every module's prediction is evaluated against its ground truth.
For readability, modules are numbered: 1, 2, 31, 32 and 4.

\begin{description}
\item[\textbf{SelectMotifMlp (1)}]
\begin{equation}
    \mathcal{L}_1 = CrossEntropy(\boldsymbol{y_1}, \boldsymbol{\hat{y_1}})
\end{equation}
Where $\boldsymbol{y_1}$ is the prediction: a normalized vector representing a distribution over the Motif vocabulary.
$\boldsymbol{\hat{y_1}}$ is a one-hot vector representing the ground truth.

\item[\textbf{SelectAttachmentClusters (2)}]
\begin{equation}
    \mathcal{L}_2 = BinaryCrossEntropy(\boldsymbol{y_2}, \boldsymbol{\hat{y_2}})
\end{equation}
Where $\boldsymbol{y_2}$ is the prediction: a binary vector indicating whether a mgraph's node (or cluster) is eligible or not for attachment with the Motif \footnotemark[\value{footnote}].
$\boldsymbol{\hat{y_2}}$ is the ground truth.

\item[\textbf{SelectAttachmentAtom(cluster1) (31)}]
\begin{equation}
    \mathcal{L}_{31} = BinaryCrossEntropy(\boldsymbol{y_{31}}, \boldsymbol{\hat{y_{31}}})
\end{equation}

Where $\boldsymbol{y_{31}}$ is the prediction: a binary vector indicating whether a cluster1's atom is eligible or not for attachment with the Motif \footnotemark[\value{footnote}].
$\boldsymbol{\hat{y_{31}}}$ is the ground truth


\item[\textbf{SelectAttachmentAtom(motif) (32)}]
\begin{equation}
    \mathcal{L}_{32} = BinaryCrossEntropy(\boldsymbol{y_{32}}, \boldsymbol{\hat{y_{32}}})
\end{equation}

Where $\boldsymbol{y_{32}}$ is the prediction: a binary vector indicating whether a motif's atom is eligible or not for attachment with cluster1 \footnotemark{\value{footnote}}.
$\boldsymbol{\hat{y_{32}}}$ is the ground truth.


\item[\textbf{SelectAttachmentBondType (4)}]
\begin{equation}
    \mathcal{L}_4 = CrossEntropy(\boldsymbol{y_4}, \boldsymbol{\hat{y_4}})
\end{equation}

Where $\boldsymbol{y_{4}}$ is the prediction: a distribution over the 4 possible bond types: none \footnote{''None'' is deprecated, labels won't ever have this value}, single, double and triple covalent.
$\boldsymbol{\hat{y_{4}}}$ is a one-hot vector indicating the ground truth.

\end{description}

The final loss function is the weighted sum of the contributes provided by each module:

\begin{equation}
\begin{gathered}
\mathcal{L} = \alpha_1 * \mathcal{L}_1 +
    \alpha_2 * \mathcal{L}_2 +
    \alpha_{31} * \mathcal{L}_{31} +
    \alpha_{32} * \mathcal{L}_{32} +
    \alpha_4 * \mathcal{L}_4
\end{gathered}
\end{equation}

\footnotetext{Note that the eligible atoms could be more than one because of graph automorphisms (there could be atoms that play the same role within the graph)}

\section{Evaluation}

During training, Molgena is repeatedly evaluated with a set of molecules sampled from the test set.
The evaluation is performed at different levels: at module level, and model level.

\subsection{Module level evaluation}

At module level, we check the accuracy of each prediction against the labels for one attachment step:

\begin{description}

\item[\textbf{SelectMotifMlp (1)}]
\begin{equation}
    \m_1 = \frac{n_{correct}}{n_{total}}
\end{equation}

Considering a batch, the accuracy is the number of correctly predicted motifs over the batch size.

\item{\textbf{SelectAttachmentClusters (2)}}
\begin{equation}
    \m_2 =
    \frac{|\boldsymbol{c_{pred}} \intersection \boldsymbol{c_{label}}|}
    {|\boldsymbol{c_{pred}} \union \boldsymbol{c_{label}}|}
\end{equation}

The accuracy is the IOU (Intersection Over Union) of the predicted clusters and the labels.

\item{\textbf{SelectAttachmentAtom(cluster1) (31)}}
\begin{equation}
    \m_{31} =
    \frac{|\boldsymbol{a_{pred}} \intersection \boldsymbol{a_{label}}|}
    {|\boldsymbol{a_{pred}} \union \boldsymbol{a_{label}}|}
\end{equation}

The accuracy is the IOU of the predicted cluster1 atoms and the labels.

\item{\textbf{SelectAttachmentAtom(motif) (32)}}
\begin{equation}
    \m_{32} =
    \frac{|\boldsymbol{a_{pred}} \intersection \boldsymbol{a_{label}}|}
    {|\boldsymbol{a_{pred}} \union \boldsymbol{a_{label}}|}
\end{equation}

The accuracy is the IOU of the predicted cluster1 atoms and the labels.

\item{\textbf{SelectAttachmentBondType (4)}}
\begin{equation}
    \m_4 = \frac{n_{correct}}{n_{total}}
\end{equation}

Considering a batch, the accuracy is the number of correctly predicted bond types over the batch size.

\end{description}

\subsection{Model level evaluation (or ''complete inference'')}

The evaluation for the reconstruction task is also called model level evaluation (or ''complete inference'' internally).

This evaluation consists in drawing a molecule from the test set, albeit the target molecule.
And inference all Molgena modules to reconstruct the target molecule from scratch.

It's an iterative process that ends when the END token is selected by the SelectMotifMlp module, 

\begin{lstlisting}
tmol = "target molecule"
pmol = ""

SelectMotifMlp(pmol, tmol)
z_pmol = EncodeMol(pmol)
z_tmol = EncodeMol(tmol)
EncodeMol(pmol_mgraph)

motif = SelectMotifMlp(pmol)
z_motif = EncodeMol(motif)

cluster1 = SelectAttachmentClusters(pmol_mgraph, motif.feature_vector)
cluster1_atom = SelectAttachmentAtom(cluster1, z_motif, z_tmol)
cluster2_atom = SelectAttachmentAtom(cluster2, repr(cluster1), z_tmol)
bond_type = SelectAttachmentBondType(cluster1_atom, cluster2_atom)
\end{lstlisting}


\section{Future work}

\begin{itemize}
\item Increase the accuracy of the reconstruction task, SOTA for similar models is >70\%
\item Add target molecule latent vector to input to SelectAttachmentClusters
\item Improve ''complete inference'' with parallelization
\end{itemize}


\end{document}
