import multiprocess 
from concurrent.futures import ThreadPoolExecutor
from math import ceil
from time import time


class FastIterator:
    def __init__(self, process_pool: multiprocess.Pool, thread_pool: ThreadPoolExecutor):
        self._process_pool = process_pool
        self._thread_pool = thread_pool


    @staticmethod
    def _iterate_chunk(proc_idx: int, from_idx: int, to_idx: int, callback):
        assert(from_idx <= to_idx)

        generated_data = []
        num_items = to_idx - from_idx
        last_log_t = time()
        for i in range(from_idx, to_idx):
            generated_data_entry = callback(i)
            if generated_data_entry != None:
                generated_data.append((i, generated_data_entry))
            if time() - last_log_t > 5.0:
                print(f"Worker {proc_idx:>3} - Processed {i - from_idx:>4}/{num_items:>4}...")
                last_log_t = time()
        print(f"Worker {proc_idx:>3} - Work done")
        return generated_data


    @staticmethod
    def _process_generated_data(job_idx: int, generated_data: list, process_callback):
        print(f"Job {job_idx}; Processing {len(generated_data)}...")
        last_log_t = time()
        processed_data = []
        for i, generated_data_entry in generated_data:
            processed_data_entry = process_callback(i, generated_data_entry);
            if processed_data_entry != None:
                processed_data.append(processed_data_entry)
            if time() - last_log_t > 5.0:
                print(f"Job {job_idx:>3} - Processed {i}/{len(generated_data)}...")
                last_log_t = time()
        print(f"Job {job_idx}; Work done")
        return processed_data


    def iterate(self, num_items: int, generate_callback, process_callback):
        """
        Iterate the until num_items in a multiprocess manner.
        Invoke generate_callback for every item, from different processes. Such callback possibly can generate data.
        Optional: invoke process_callback on the generated results; the invocations will be on the same process. 
        """

        num_processes = self._process_pool._processes
        chunk_size = ceil(num_items / num_processes)

        # Iterate items and possibly generate data using multiple processes
        generated_async_list = []
        for proc_idx in range(0, num_processes):
            from_idx = proc_idx * chunk_size
            to_idx = min(from_idx + chunk_size, num_items)
            generated_async = self._process_pool.apply_async(FastIterator._iterate_chunk, (proc_idx, from_idx, to_idx, generate_callback))
            generated_async_list.append(generated_async)
        
        # Process previously generated data using multiple threads but one process
        # Useful e.g. if we want to update one block of memory (dataframe) with data generated by other processes
        future_processed_list = []
        for job_idx, generated_async in enumerate(generated_async_list):
            generated_data = generated_async.get()
            if process_callback and generated_data:
                future_processed = self._thread_pool.submit(FastIterator._process_generated_data, job_idx, generated_data, process_callback)
                future_processed_list.append(future_processed)

        # Finally processed data is gathered in one list
        processed_list = []
        for future_processed in future_processed_list:
            processed_list.extend(future_processed.result())
        return processed_list

